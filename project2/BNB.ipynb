{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import time\n",
    "from sklearn import metrics\n",
    "path='dataset//reddit_train.csv'\n",
    "path_test='dataset//reddit_test.csv'\n",
    "labels=['Overwatch','Music','conspiracy','AskReddit','worldnews','anime','hockey','baseball','funny','nba',\n",
    "    'canada','soccer','nfl','trees','gameofthrones','wow','leagueoflegends','GlobalOffensive','europe','movies']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comments</th>\n",
       "      <th>subreddits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Honestly, Buffalo is the correct answer. I rem...</td>\n",
       "      <td>hockey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Ah yes way could have been :( remember when he...</td>\n",
       "      <td>nba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>https://youtu.be/6xxbBR8iSZ0?t=40m49s\\n\\nIf yo...</td>\n",
       "      <td>leagueoflegends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>He wouldn't have been a bad signing if we woul...</td>\n",
       "      <td>soccer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Easy. You use the piss and dry technique. Let ...</td>\n",
       "      <td>funny</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                           comments       subreddits\n",
       "0   0  Honestly, Buffalo is the correct answer. I rem...           hockey\n",
       "1   1  Ah yes way could have been :( remember when he...              nba\n",
       "2   2  https://youtu.be/6xxbBR8iSZ0?t=40m49s\\n\\nIf yo...  leagueoflegends\n",
       "3   3  He wouldn't have been a bad signing if we woul...           soccer\n",
       "4   4  Easy. You use the piss and dry technique. Let ...            funny"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(path)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'comments', 'subreddits'], dtype='object')\n",
      "(70000, 3)\n",
      "   id                                           comments       subreddits\n",
      "0   0  Honestly, Buffalo is the correct answer. I rem...           hockey\n",
      "1   1  Ah yes way could have been :( remember when he...              nba\n",
      "2   2  https://youtu.be/6xxbBR8iSZ0?t=40m49s\\n\\nIf yo...  leagueoflegends\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AskReddit          3500\n",
       "Music              3500\n",
       "GlobalOffensive    3500\n",
       "worldnews          3500\n",
       "conspiracy         3500\n",
       "soccer             3500\n",
       "wow                3500\n",
       "movies             3500\n",
       "trees              3500\n",
       "Overwatch          3500\n",
       "leagueoflegends    3500\n",
       "nba                3500\n",
       "nfl                3500\n",
       "baseball           3500\n",
       "gameofthrones      3500\n",
       "anime              3500\n",
       "europe             3500\n",
       "funny              3500\n",
       "canada             3500\n",
       "hockey             3500\n",
       "Name: subreddits, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data.columns)                         \n",
    "print(data.shape)                           \n",
    "print(data.loc[0:2])                        \n",
    "categories=['']\n",
    "data['subreddits'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56000\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data['comments'], data['subreddits'], train_size=0.8, test_size=0.2)\n",
    "print(len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "vectors_train = vectorizer.fit_transform(X_train)\n",
    "\n",
    "vectors_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf_idf_vectorizer = TfidfVectorizer(sublinear_tf = True)\n",
    "vectors_train_idf = tf_idf_vectorizer.fit_transform(X_train)\n",
    "vectors_test_idf = tf_idf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BNB():\n",
    "    \n",
    "    def __int__(self, labels):\n",
    "        self.labels=labels\n",
    "        pass\n",
    "    \n",
    "    def prior(self, vectors_train, y_train):        \n",
    "        ##y_prior\n",
    "        y_prior=np.zeros(len(labels))\n",
    "        y_num=np.zeros(len(labels))\n",
    "        for i in range(len(y_prior)):\n",
    "            y_num[i]=y_train.value_counts()[labels[i]]\n",
    "        y_prior=y_num/sum(y_num)\n",
    "        \n",
    "        ##x_prior\n",
    "        y_train_m=y_train.as_matrix()## series to matrix\n",
    "        lens=vectors_train.shape[1]\n",
    "        features=np.zeros((len(labels),lens))\n",
    "        for (index,data) in enumerate(vectors_train):\n",
    "            a=labels.index(y_train_m[index])\n",
    "            features[a, data.indices]+=1\n",
    "        return y_prior, features, y_num\n",
    "    \n",
    "    def cond_prob(self, vectors_train, y_train):\n",
    "        y_prior, features, y_num=self.prior(vectors_train, y_train)\n",
    "        y_num_sm=y_num+2  #smoothing\n",
    "        features_sm=features+1\n",
    "        lens=vectors_train.shape[1]\n",
    "        \n",
    "        theta_1=np.zeros((len(labels),lens))\n",
    "        theta_0=np.zeros((len(labels),lens))\n",
    "\n",
    "        x_sum=np.zeros(lens)#total # of xj\n",
    "        y_sum=sum(y_num)#total # of samples\n",
    "\n",
    "        for i in range(lens):\n",
    "            x_sum[i]=sum(features[:,i])\n",
    "\n",
    "        for i in range(len(labels)):\n",
    "            theta_1[i]=features_sm[i]/y_num_sm[i]\n",
    "            theta_0[i]=(x_sum-features[i]+1)/(y_sum-y_num[i]+2)\n",
    "            \n",
    "        return theta_1, theta_0, y_prior\n",
    "    \n",
    "    def fit(self, vectors_train, y_train):\n",
    "        lens=vectors_train.shape[1]\n",
    "        theta_1, theta_0, y_prior=self.cond_prob(vectors_train, y_train)\n",
    "        wj_1=np.zeros((len(labels),lens))\n",
    "        wj_0=np.zeros((len(labels),lens))\n",
    "        wj_1=np.log(theta_1)-np.log(theta_0)\n",
    "        wj_0=np.log(1-theta_1)-np.log(1-theta_0)\n",
    "        \n",
    "        w0=np.zeros(len(labels))\n",
    "        w0=np.log(y_prior)-np.log(1-y_prior)+wj_0.sum(axis=1)\n",
    "        return w0, wj_1, wj_0\n",
    "    \n",
    "    def predict(self, vectors_train, y_train, vectors_test):\n",
    "        w0, wj_1, wj_0=self.fit(vectors_train, y_train)\n",
    "        m,n=vectors_test.shape\n",
    "        feature_log=np.zeros(len(labels))\n",
    "        y_pre=np.zeros(m)\n",
    "        xx_binary=np.zeros(n)\n",
    "        for (index,data) in enumerate(vectors_test):\n",
    "            xx_binary[:]=0 ##binarize features\n",
    "            xx_binary[data.indices]=1\n",
    "            X_w=np.dot(wj_1-wj_0,xx_binary)\n",
    "            feature_log=w0+X_w\n",
    "            y_pre[index]=np.argmax(feature_log)\n",
    "\n",
    "        return y_pre\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:16: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time cost(s):  110.97039556503296\n",
      "test accuracy: 53.28%\n"
     ]
    }
   ],
   "source": [
    "time_start=time.time()\n",
    "clf = BNB()\n",
    "y_pre=clf.predict(vectors_train, y_train, vectors_test)\n",
    "time_end=time.time()\n",
    "print('time cost(s): ',time_end-time_start)\n",
    "labels_array=np.array(labels)\n",
    "accuracy=(labels_array[y_pre.astype(np.int)]==y_test).astype(np.int).mean()\n",
    "print(\"test accuracy: %0.2f%%\" %(accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 49.71%\n",
      "time cost(s):  0.39664673805236816\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "time_start=time.time()\n",
    "clf_B = BernoulliNB()\n",
    "clf_B.fit(vectors_train, y_train)\n",
    "y_pred_B=clf_B.predict(vectors_test)\n",
    "time_end=time.time()\n",
    "print(\"test accuracy: %0.2f%%\" %((y_pred_B==y_test).astype(np.int).mean()*100))\n",
    "print('time cost(s): ',time_end-time_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
