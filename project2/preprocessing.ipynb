{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "#import numpy as np\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "path='dataset//reddit_train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comments</th>\n",
       "      <th>subreddits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Honestly, Buffalo is the correct answer. I rem...</td>\n",
       "      <td>hockey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Ah yes way could have been :( remember when he...</td>\n",
       "      <td>nba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>https://youtu.be/6xxbBR8iSZ0?t=40m49s\\n\\nIf yo...</td>\n",
       "      <td>leagueoflegends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>He wouldn't have been a bad signing if we woul...</td>\n",
       "      <td>soccer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Easy. You use the piss and dry technique. Let ...</td>\n",
       "      <td>funny</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                           comments       subreddits\n",
       "0   0  Honestly, Buffalo is the correct answer. I rem...           hockey\n",
       "1   1  Ah yes way could have been :( remember when he...              nba\n",
       "2   2  https://youtu.be/6xxbBR8iSZ0?t=40m49s\\n\\nIf yo...  leagueoflegends\n",
       "3   3  He wouldn't have been a bad signing if we woul...           soccer\n",
       "4   4  Easy. You use the piss and dry technique. Let ...            funny"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(path)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "//anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data['comments'], data['subreddits'], train_size=0.8, test_size=0.2)\n",
    "\n",
    "XX_train=X_train.as_matrix()\n",
    "XX_test=X_test.as_matrix()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# sentiment feature generator\\n\\nfrom textblob import TextBlob\\n\\ndef sentiment_generator(sentence):\\n    \\n    testimony = TextBlob(sentence)\\n    \\n    return (testimony.sentiment.polarity,testimony.sentiment.subjectivity)\\n    #return testimony.sentiment.polarity\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# sentiment feature generator\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "def sentiment_generator(sentence):\n",
    "    \n",
    "    testimony = TextBlob(sentence)\n",
    "    \n",
    "    return (testimony.sentiment.polarity,testimony.sentiment.subjectivity)\n",
    "    #return testimony.sentiment.polarity\n",
    "'''\n",
    "#(polar,sub) = sentiment_generator('I am blue')\n",
    "#print(polar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "## implementation of sentiment\n",
    "row_train = len(XX_train)\n",
    "row_test = len(XX_test)\n",
    "\n",
    "X_train_senti = np.zeros((row_train,2))\n",
    "X_test_senti = np.zeros((row_test,2))\n",
    "\n",
    "for i in range(row_train):\n",
    "    (X_train_senti[i,0],X_train_senti[i,1]) = sentiment_generator(XX_train[i])    \n",
    "\n",
    "for i in range(row_test):\n",
    "    (X_test_senti[i,0],X_test_senti[i,1]) = sentiment_generator(XX_test[i])\n",
    "\n",
    "for i in range(row_train):\n",
    "    X_train_senti[i,0] = sentiment_generator(XX_train[i])    \n",
    "\n",
    "for i in range(row_test):\n",
    "    X_test_senti[i,0] = sentiment_generator(XX_test[i])\n",
    "#print(X_train_senti)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):     \n",
    "        return wordnet.ADJ      # 'a'\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB     # 'v'\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN     # 'n'\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV      # 'r'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def lemmatize(sentence):\n",
    "\n",
    "    sen = sentence.lower()\n",
    "    \n",
    "    tokens = word_tokenize(sen)  # seperate given sentence into words\n",
    "    tagged_sent = pos_tag(tokens)     # get the part of speech of each word\n",
    "                                      # tagged_sent in form of (word,tag) pair list\n",
    "\n",
    "    wnl = WordNetLemmatizer()\n",
    "    lemmas_sent = ''\n",
    "    for tag in tagged_sent:\n",
    "        wordnet_pos = get_wordnet_pos(tag[1]) or wordnet.NOUN\n",
    "        if wordnet_pos == wordnet.ADV :\n",
    "            continue\n",
    "        else:\n",
    "            lemmas_sent = lemmas_sent + (wnl.lemmatize(tag[0], pos=wordnet_pos))+' ' # Lemmatization\n",
    "        \n",
    "    return lemmas_sent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## implementation of lemmatization\n",
    "X_train_lem = np.copy(XX_train)\n",
    "for i in range(XX_train.shape[0]):\n",
    "    X_train_lem[i] = lemmatize(XX_train[i])\n",
    "    \n",
    "#X_train_lem\n",
    "\n",
    "X_test_lem = np.copy(XX_test)\n",
    "for i in range(XX_test.shape[0]):\n",
    "    X_test_lem[i] = lemmatize(XX_test[i])\n",
    "\n",
    "#X_test_lem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectors_train_idf\n",
      "   (0, 13540)\t0.4764922030612956\n",
      "  (0, 58910)\t0.11385014526718887\n",
      "  (0, 30129)\t0.4526114478587218\n",
      "  (0, 6660)\t0.18511674475275605\n",
      "  (0, 58341)\t0.18262855597870836\n",
      "  (0, 28876)\t0.38237522181887373\n",
      "  (0, 48125)\t0.4785314715160959\n",
      "  (0, 64563)\t0.33512926457196546\n",
      "  (1, 58459)\t0.22821680265093738\n",
      "  (1, 8198)\t0.33069233473916765\n",
      "  (1, 26621)\t0.2444172421918026\n",
      "  (1, 28255)\t0.22595645035712045\n",
      "  (1, 5274)\t0.5849633598913054\n",
      "  (1, 51826)\t0.20476084526150187\n",
      "  (1, 22167)\t0.32754329416234984\n",
      "  (1, 40973)\t0.13902443459453584\n",
      "  (1, 18116)\t0.22069842641514276\n",
      "  (1, 29266)\t0.40014353563780264\n",
      "  (1, 58910)\t0.09066080369420935\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "\n",
    "tf_idf_vectorizer = TfidfVectorizer(sublinear_tf = True)\n",
    "\n",
    "vectors_train_idf = tf_idf_vectorizer.fit_transform(X_train)\n",
    "vectors_test_idf = tf_idf_vectorizer.transform(X_test)\n",
    "\n",
    "vectors_train_lem_idf = tf_idf_vectorizer.fit_transform(X_train_lem.tolist())\n",
    "vectors_test_lem_idf = tf_idf_vectorizer.transform(X_test_lem.tolist())\n",
    "\n",
    "print(\"vectors_train_idf\\n\",vectors_train_idf[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56000, 65959)\n"
     ]
    }
   ],
   "source": [
    "print(vectors_train_idf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF without using Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.56830552 0.55769746 0.5640957  0.56346583 0.56134394]\n"
     ]
    }
   ],
   "source": [
    "### Cross Validation + MultinomialNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "MNB_clf = MultinomialNB(alpha=.2)\n",
    "MNB_clf.fit(vectors_train_idf, y_train)\n",
    "scores = cross_val_score(MNB_clf, vectors_train_idf, y_train, cv=5)\n",
    "print(scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF with Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.566164   0.55760821 0.5661489  0.56310853 0.55759092]\n"
     ]
    }
   ],
   "source": [
    "### Cross Validation + MultinomialNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "MNB_clf = MultinomialNB(alpha=.2)\n",
    "MNB_clf.fit(vectors_train_lem_idf, y_train)\n",
    "scores = cross_val_score(MNB_clf, vectors_train_lem_idf, y_train, cv=5)\n",
    "print(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
